








from scipy.io import loadmat
import matplotlib.pyplot as plt
import numpy as np

dati = loadmat('test_7_maggio_2025')
A = dati["A"]
A = A.astype(float)
b = dati["b"]
b = b.astype(float)
A1 = dati["A1"]
A1 = A1.astype(float)
b1 = dati["b1"]
b1 = b1.astype(float)


m, n = A.shape
print("Dimensione A:", m, "x", n)
sp = np.count_nonzero(A) / (m * n)
print("Percentuale sparsità", sp)


flagS = A == A.T # A è simmetrica?

if np.all(flagS):
    print("A Matrice simmetrica")
else:
    print("A Matrice non simmetrica")


eigenvalue = np.linalg.eigvals(A) # A è def pos?

if np.all(eigenvalue) > 0:
    print("A Matrice definita positiva")


# Indice di condizionamento
condA = np.linalg.cond(A)
print("Indice di condizionamento A:", condA)


def steepestdescent(A, b, x0, itmax, tol):
    n, m = A.shape
    if n != m:
        print("Matrice non quadrata")
        return [], []    
    
    # inizializzare le variabili necessarie
    x = x0
    r = A@x - b
    p = -r
    it = 0
    nb = np.linalg.norm(b)
    errore = np.linalg.norm(r) / nb
    vec_sol = []
    vec_sol.append(x.copy())
    vet_r = []
    vet_r.append(errore)
     
    # utilizzare il metodo del gradiente per trovare la soluzione
    while errore >= tol and it < itmax:
        it = it+1
        Ap = A@p
        alpha = -(r.T@p) / (p.T@Ap)
        x = x + alpha * p
         
        vec_sol.append(x.copy())
        r = r + alpha * Ap
        errore = np.linalg.norm(r) / nb
        vet_r.append(errore)
        p = -r
        
    iterates_array = np.vstack([arr.T for arr in vec_sol])
    return x, vet_r, iterates_array, it


def conjugate_gradient(A, b, x0, itmax, tol):
    n, m = A.shape
    if n != m:
        print("Matrice non quadrata")
        return [], []    
    
    # inizializzare le variabili necessarie
    x = x0
    r = A@x - b
    p = -r
    it = 0
    nb = np.linalg.norm(b)
    errore = np.linalg.norm(r) / nb
    vec_sol = []
    vec_sol.append(x.copy())
    vet_r = []
    vet_r.append(errore)
     
    # utilizzare il metodo del gradiente per trovare la soluzione
    while errore >= tol and it < itmax:
        it = it+1
        Ap = A@p
        alpha = -(r.T@p) / (p.T@Ap)
        x = x + alpha * p
         
        vec_sol.append(x.copy())
        rtr_old = r.T@r
        r = r + alpha * Ap
        errore = np.linalg.norm(r) / nb
        vet_r.append(errore)
        gamma = r.T@r / rtr_old
        p = -r + gamma * p
        
    iterates_array = np.vstack([arr.T for arr in vec_sol])
    return x, vet_r, iterates_array, it





x0 = np.zeros_like(b)
tol = 1e-6
itmax = 2000
x, vet_r, iterates_array, it = steepestdescent(A, b, x0, itmax, tol)
print("Numero iterazioni", it)
plt.semilogy(range(len(vet_r)), vet_r)
plt.show()
xCG, vet_rCG, iterates_arrayCG, itCG = conjugate_gradient(A, b, x0, itmax, tol)
print("Numero iterazioni", itCG)
plt.semilogy(range(len(vet_rCG)), vet_rCG)
plt.show()


condA1 = np.linalg.cond(A1)
print("Indice di condizionamento A1:", condA1)


x0_1 = np.zeros_like(b1)
x1, vet_r1, iterates_array1, it1 = steepestdescent(A1, b1, x0_1, itmax, tol)
print("Numero iterazioni", it1)
plt.semilogy(range(len(vet_r1)), vet_r1)
plt.show()

x1CG, vet_r1CG, iterates_array1CG, it1CG = conjugate_gradient(A1, b1, x0_1, itmax, tol)
print("Numero iterazioni", it1CG)
plt.semilogy(range(len(vet_r1CG)), vet_r1CG)
plt.show()




































